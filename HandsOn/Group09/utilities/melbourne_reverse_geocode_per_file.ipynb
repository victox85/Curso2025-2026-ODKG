{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a507b8c3",
   "metadata": {},
   "source": [
    "\n",
    "# 🗺️ MelbSensors ⇢ Reverse Geocode por archivo (CSV a CSV)  \n",
    "**Objetivo:** Tomar **cada** archivo (CSV o JSON) que tenga coordenadas y devolver **un CSV enriquecido por archivo**, conservando **todas** las columnas originales y agregando:  \n",
    "`street`, `housenumber`, `postcode`, `city`, `country`, `formatted`.\n",
    "\n",
    "### ✅ Qué hace este notebook\n",
    "- Soporta **CSV** con `latitude/longitude` o `location=[lat, lon]` (y variantes) y **JSON** con `{\"records\":[{\"fields\":...}]}`.  \n",
    "- Detecta separador del CSV automáticamente y muestra **diagnósticos** (columnas, filas, cómo detectó lat/lon).  \n",
    "- Enriquecer **por archivo** → guarda `*_enriched.csv` con las nuevas columnas al final.  \n",
    "- Incluye **cargador de archivos** de Colab y **montaje de Drive** (opcional).\n",
    "\n",
    "> **Tip**: si tu CSV tiene otras cabeceras (p.ej. `Latitude`/`Longitude` con mayúsculas o `latitud`/`longitud` en español), este notebook las detecta. Si no, podés forzar el mapeo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c0c13",
   "metadata": {},
   "source": [
    "\n",
    "## 🔌 (Opcional) Montar Google Drive y/o subir archivos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ebfec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejecuta esta celda si quieres usar Drive\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Subir archivos manualmente (si no los tienes en /content o Drive)\n",
    "try:\n",
    "    from google.colab import files  # type: ignore\n",
    "    # Descomenta para abrir el selector de archivos\n",
    "    # uploaded = files.upload()\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7651be3",
   "metadata": {},
   "source": [
    "\n",
    "## ⚙️ Configuración\n",
    "- Pon tu **API key** de Geoapify (o define la variable de entorno `GEOAPIFY_API_KEY`).  \n",
    "- Lista los archivos en `SOURCES`.  \n",
    "- Si tu CSV usa encabezados no estándar, puedes **forzar** las columnas en `FORCE_COLUMN_MAP`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, math\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# API key (podés setearla por entorno o pegarla aquí)\n",
    "GEOAPIFY_API_KEY = os.environ.get(\"GEOAPIFY_API_KEY\", \"\").strip() or \"c11f80da9db5447fbd62cee763865106\"\n",
    "assert GEOAPIFY_API_KEY, \"Falta GEOAPIFY_API_KEY\"\n",
    "\n",
    "# Archivos/URLs a procesar: se crea un *_enriched.csv por cada uno\n",
    "SOURCES = [\n",
    "    # Ejemplos:\n",
    "    # \"/content/microclimate-sensor-locations.csv\",\n",
    "    # \"/content/pedestrian-counting-system-sensor-locations.json\",\n",
    "]\n",
    "\n",
    "# ⚠️ Solo si necesitás forzar nombres de columnas (por archivo)\n",
    "# Claves: 'lat', 'lon' y opcionalmente 'location' (lista [lat, lon]).\n",
    "# Usa el nombre EXACTO de la columna tal como aparece en tu CSV/JSON.\n",
    "FORCE_COLUMN_MAP = {\n",
    "    # \"/content/microclimate-sensor-locations.csv\": {\"lat\": \"Latitude\", \"lon\": \"Longitude\"},\n",
    "    # \"/content/pedestrian-counting-system-sensor-locations.csv\": {\"location\": \"Location\"},\n",
    "}\n",
    "\n",
    "# Mínimo delay entre llamadas para ser amable con la API\n",
    "REVERSE_SLEEP_SEC = 0.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673ae64",
   "metadata": {},
   "source": [
    "\n",
    "## 🧰 Utilidades (carga robusta, detección de columnas, reverse geocoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "def _is_url(path: str) -> bool:\n",
    "    try:\n",
    "        return urlparse(path).scheme in {\"http\", \"https\"}\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def parse_json_records(data) -> pd.DataFrame:\n",
    "    # JSON estilo {\"records\":[{\"fields\": {...}}]}\n",
    "    if isinstance(data, dict) and \"records\" in data:\n",
    "        rows = []\n",
    "        for rec in data.get(\"records\", []):\n",
    "            fields = rec.get(\"fields\", {})\n",
    "            if isinstance(fields, dict):\n",
    "                rows.append(fields)\n",
    "        return pd.json_normalize(rows)\n",
    "    # JSON lista de dicts\n",
    "    if isinstance(data, list) and all(isinstance(x, dict) for x in data):\n",
    "        return pd.json_normalize(data)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def read_csv_smart(path_or_url: str) -> pd.DataFrame:\n",
    "    # Intentar con separador automático y diferentes encodings\n",
    "    for sep in [None, \",\", \";\", \"|\", \"\t\"]:\n",
    "        for enc in [\"utf-8\", \"utf-8-sig\", \"latin-1\"]:\n",
    "            try:\n",
    "                return pd.read_csv(path_or_url, sep=sep, engine=\"python\", encoding=enc)\n",
    "            except Exception:\n",
    "                continue\n",
    "    # último intento por defecto\n",
    "    return pd.read_csv(path_or_url)\n",
    "\n",
    "def load_any(path_or_url: str) -> pd.DataFrame:\n",
    "    if _is_url(path_or_url):\n",
    "        # Intentar JSON\n",
    "        try:\n",
    "            r = requests.get(path_or_url, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            try:\n",
    "                return parse_json_records(r.json())\n",
    "            except Exception:\n",
    "                pass\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Intentar CSV\n",
    "        return read_csv_smart(path_or_url)\n",
    "    else:\n",
    "        if not os.path.exists(path_or_url):\n",
    "            raise FileNotFoundError(f\"No existe: {path_or_url}\")\n",
    "        if path_or_url.lower().endswith(\".json\"):\n",
    "            with open(path_or_url, \"r\", encoding=\"utf-8\") as f:\n",
    "                return parse_json_records(json.load(f))\n",
    "        if path_or_url.lower().endswith(\".csv\"):\n",
    "            return read_csv_smart(path_or_url)\n",
    "        # Heurística\n",
    "        try:\n",
    "            with open(path_or_url, \"r\", encoding=\"utf-8\") as f:\n",
    "                return parse_json_records(json.load(f))\n",
    "        except Exception:\n",
    "            return read_csv_smart(path_or_url)\n",
    "\n",
    "def extract_lat_lon_columns(df: pd.DataFrame, forced: dict | None = None):\n",
    "    \"\"\"\n",
    "    Devuelve (lat, lon, used_from) como Series y etiqueta de origen ('latitude/longitude','location','forced','variants').\n",
    "    Si no encuentra, devuelve (None, None, 'none').\n",
    "    \"\"\"\n",
    "    if forced:\n",
    "        # Forzar mapeo\n",
    "        if \"location\" in forced and forced[\"location\"] in df.columns:\n",
    "            loc_col = forced[\"location\"]\n",
    "            lat, lon = _latlon_from_location_series(df[loc_col])\n",
    "            return lat, lon, \"forced(location)\"\n",
    "        if \"lat\" in forced and \"lon\" in forced and forced[\"lat\"] in df.columns and forced[\"lon\"] in df.columns:\n",
    "            lat = pd.to_numeric(df[forced[\"lat\"]], errors=\"coerce\")\n",
    "            lon = pd.to_numeric(df[forced[\"lon\"]], errors=\"coerce\")\n",
    "            return lat, lon, \"forced(lat/lon)\"\n",
    "\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    # Caso directo: latitude/longitude (insensible a mayúsculas)\n",
    "    if \"latitude\" in lower and \"longitude\" in lower:\n",
    "        lat = pd.to_numeric(df[lower[\"latitude\"]], errors=\"coerce\")\n",
    "        lon = pd.to_numeric(df[lower[\"longitude\"]], errors=\"coerce\")\n",
    "        return lat, lon, \"latitude/longitude\"\n",
    "\n",
    "    # Desde 'location' (lista [lat, lon] o [lon, lat])\n",
    "    if \"location\" in lower:\n",
    "        lat, lon = _latlon_from_location_series(df[lower[\"location\"]])\n",
    "        return lat, lon, \"location\"\n",
    "\n",
    "    # Variantes comunes\n",
    "    lat_col = None\n",
    "    for a in [\"lat\", \"latitude_deg\", \"Lat\", \"Latitude\"]:\n",
    "        if a in df.columns:\n",
    "            lat_col = a\n",
    "            break\n",
    "        if a.lower() in lower:\n",
    "            lat_col = lower[a.lower()]\n",
    "            break\n",
    "    lon_col = None\n",
    "    for b in [\"lon\", \"lng\", \"longitude_deg\", \"Long\", \"Longitude\", \"Lng\"]:\n",
    "        if b in df.columns:\n",
    "            lon_col = b\n",
    "            break\n",
    "        if b.lower() in lower:\n",
    "            lon_col = lower[b.lower()]\n",
    "            break\n",
    "\n",
    "    if lat_col and lon_col:\n",
    "        lat = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
    "        lon = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
    "        return lat, lon, \"variants\"\n",
    "\n",
    "    return None, None, \"none\"\n",
    "\n",
    "def _latlon_from_location_series(series: pd.Series):\n",
    "    def _extract(x):\n",
    "        if isinstance(x, (list, tuple)) and len(x) >= 2:\n",
    "            a, b = x[0], x[1]\n",
    "            if isinstance(a, (int, float)) and isinstance(b, (int, float)):\n",
    "                if -90 <= a <= 90 and -180 <= b <= 180:\n",
    "                    return a, b\n",
    "                if -90 <= b <= 90 and -180 <= a <= 180:\n",
    "                    return b, a\n",
    "        # Si llega como string tipo \"[lat, lon]\"\n",
    "        if isinstance(x, str) and \"[\" in x and \"]\" in x:\n",
    "            try:\n",
    "                vals = json.loads(x)\n",
    "                if isinstance(vals, list) and len(vals) >= 2:\n",
    "                    a, b = vals[0], vals[1]\n",
    "                    if -90 <= a <= 90 and -180 <= b <= 180:\n",
    "                        return a, b\n",
    "                    if -90 <= b <= 90 and -180 <= a <= 180:\n",
    "                        return b, a\n",
    "            except Exception:\n",
    "                pass\n",
    "        return (math.nan, math.nan)\n",
    "    ll = series.apply(_extract)\n",
    "    lat = pd.Series([v[0] for v in ll], index=series.index)\n",
    "    lon = pd.Series([v[1] for v in ll], index=series.index)\n",
    "    return lat, lon\n",
    "\n",
    "def reverse_geocode(lat: float, lon: float, api_key: str, session: requests.Session, max_retries: int = 3):\n",
    "    url = \"https://api.geoapify.com/v1/geocode/reverse\"\n",
    "    params = {\"lat\": float(lat), \"lon\": float(lon), \"format\": \"json\", \"apiKey\": api_key}\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            r = session.get(url, params=params, timeout=30)\n",
    "            if r.status_code == 200:\n",
    "                d = r.json()\n",
    "                if isinstance(d, dict) and d.get(\"results\"):\n",
    "                    return d[\"results\"][0]\n",
    "                return None\n",
    "            if r.status_code in (429, 500, 503):\n",
    "                sleep(1.5 * attempt)  # backoff simple\n",
    "            else:\n",
    "                return None\n",
    "        except requests.RequestException:\n",
    "            sleep(1.5 * attempt)\n",
    "    return None\n",
    "\n",
    "def addr_fields(res: dict) -> dict:\n",
    "    return {\n",
    "        \"street\": res.get(\"street\") if res else None,\n",
    "        \"housenumber\": res.get(\"housenumber\") if res else None,\n",
    "        \"postcode\": res.get(\"postcode\") if res else None,\n",
    "        \"city\": (res.get(\"city\") or res.get(\"district\") or res.get(\"county\")) if res else None,\n",
    "        \"country\": res.get(\"country\") if res else None,\n",
    "        \"formatted\": res.get(\"formatted\") if res else None,\n",
    "    }\n",
    "\n",
    "def stem_for_output(path_or_url: str) -> str:\n",
    "    if _is_url(path_or_url):\n",
    "        parsed = urlparse(path_or_url)\n",
    "        name = os.path.basename(parsed.path) or \"dataset\"\n",
    "    else:\n",
    "        name = os.path.basename(path_or_url)\n",
    "    stem = os.path.splitext(name or \"dataset\")[0] or \"dataset\"\n",
    "    return stem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879619e",
   "metadata": {},
   "source": [
    "\n",
    "## 🔍 Diagnóstico de archivos\n",
    "Muestra: filas, columnas, primeras filas y cómo detectó las coordenadas.  \n",
    "Si ves `source: none`, ajusta `FORCE_COLUMN_MAP` para ese archivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for src in SOURCES:\n",
    "    print(f\"\\n==> Analizando: {src}\")\n",
    "    try:\n",
    "        df = load_any(src)\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error al cargar: {e}\")\n",
    "        continue\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"   ⚠️ DataFrame vacío.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"   ✔️ Shape: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
    "    print(\"   ✔️ Columnas:\", list(df.columns)[:50], (\"...\" if len(df.columns) > 50 else \"\"))\n",
    "    display(df.head(3))\n",
    "\n",
    "    forced = FORCE_COLUMN_MAP.get(src)\n",
    "    lat, lon, used_from = extract_lat_lon_columns(df, forced=forced)\n",
    "    print(f\"   🔎 Detección de coordenadas: {used_from}\")\n",
    "    if lat is not None and lon is not None:\n",
    "        valid = int((lat.notna() & lon.notna()).sum())\n",
    "        print(f\"   ✔️ Coordenadas válidas: {valid} filas\")\n",
    "    else:\n",
    "        print(\"   ⚠️ No se detectaron columnas de coordenadas. Considera FORCE_COLUMN_MAP.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833c332",
   "metadata": {},
   "source": [
    "\n",
    "## ▶️ Enriquecer y exportar (CSV por archivo)\n",
    "- Conservar todas las columnas originales; añade las nuevas al final.  \n",
    "- Si alguna fila no tiene coordenadas válidas, deja columnas nuevas vacías.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f6043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session = requests.Session()\n",
    "cache = {}\n",
    "created = []\n",
    "\n",
    "for src in SOURCES:\n",
    "    print(f\"\\n==> Procesando: {src}\")\n",
    "    try:\n",
    "        df = load_any(src)\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error al cargar: {e}\")\n",
    "        continue\n",
    "    if df.empty:\n",
    "        print(\"   ⚠️ DataFrame vacío, se omite.\")\n",
    "        continue\n",
    "\n",
    "    forced = FORCE_COLUMN_MAP.get(src)\n",
    "    lat, lon, used_from = extract_lat_lon_columns(df, forced=forced)\n",
    "    if lat is None or lon is None:\n",
    "        print(\"   ⚠️ No hay columnas de coordenadas detectadas; se guardará sin enriquecimiento.\")\n",
    "    else:\n",
    "        print(f\"   🔎 Usando columnas de coordenadas desde: {used_from}\")\n",
    "\n",
    "    # Copia exacta; columnas nuevas al final\n",
    "    out = df.copy()\n",
    "    for c in [\"street\", \"housenumber\", \"postcode\", \"city\", \"country\", \"formatted\"]:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "\n",
    "    if lat is not None and lon is not None:\n",
    "        for idx in tqdm(out.index, desc=f\"Reverse geocoding ({stem_for_output(src)})\"):\n",
    "            la = lat[idx] if isinstance(lat, pd.Series) and idx in lat.index else math.nan\n",
    "            lo = lon[idx] if isinstance(lon, pd.Series) and idx in lon.index else math.nan\n",
    "            if pd.notna(la) and pd.notna(lo):\n",
    "                key = (round(float(la), 6), round(float(lo), 6))\n",
    "                if key not in cache:\n",
    "                    res = reverse_geocode(float(la), float(lo), GEOAPIFY_API_KEY, session=session)\n",
    "                    cache[key] = addr_fields(res)\n",
    "                    sleep(REVERSE_SLEEP_SEC)\n",
    "                for k, v in cache[key].items():\n",
    "                    out.at[idx, k] = v\n",
    "\n",
    "    out_name = f\"{stem_for_output(src)}_enriched.csv\"\n",
    "    out.to_csv(out_name, index=False, encoding=\"utf-8\")\n",
    "    created.append(out_name)\n",
    "    print(f\"   ✅ Guardado: {out_name}\")\n",
    "\n",
    "print(\"\\nArchivos generados:\")\n",
    "for c in created:\n",
    "    print(\" -\", c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6ffae",
   "metadata": {},
   "source": [
    "\n",
    "### 📝 Tips si te dice “no hay nada”\n",
    "- Verifica la **ruta** exacta (`/content/...`) o que el archivo esté **subido** (usa el selector de archivos arriba).  \n",
    "- Revisa la celda de **Diagnóstico**: mira las columnas y confirma que existen `latitude/longitude` o `location`.  \n",
    "- Si tus columnas se llaman distinto, usa `FORCE_COLUMN_MAP` en Configuración:\n",
    "```python\n",
    "FORCE_COLUMN_MAP = {\n",
    "  \"/content/microclimate-sensor-locations.csv\": {\"lat\": \"Latitude\", \"lon\": \"Longitude\"},\n",
    "  # O bien si viene en una lista:\n",
    "  # \"/content/pedestrian.csv\": {\"location\": \"Location\"},\n",
    "}\n",
    "```\n",
    "- CSV con separador `;` o con BOM: el lector automático lo detecta; si ves columnas raras, abre la **Vista previa** en Diagnóstico para confirmar.\n",
    "- Si sale error 429, sube `REVERSE_SLEEP_SEC` a `0.4` o más.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
